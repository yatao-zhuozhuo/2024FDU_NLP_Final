当下，大模型飞速发展，预训练的大模型在下游具体任务上进行微调以获得出色的表现结果已经成了主流范式。目前的微调模型虽然能将预训练过程获取的知识迁移到下游任务，但在较长逻辑链推理如数学问题中表现不佳，由于推理时的累计误差导致模型最终推断错误。在本研究中，我们试图解决这一问题，针对Qwen-0.5B模型，基于MATH和GSM8K数据集进行训练和测试，我们在微调全流程（数据、训练、评测）提出了一系列创新方法。具体而言，我们进行了全局微调和基于LoRA的局部微调策略，以此作为基线。在数据层面，我们对数据集进行聚类以优化任务分配，同时借助PromptEngineering 实现了链式思维（CoT）推理。在训练层面，我们提出了LLM-Dagger，通过修正错误的思维链，来进一步提升模型能力。实验结果表明，与未经微调的Qwen-0.5B模型预测结果相比，我们提出的上述方法显著提升了模型的数学推理能力，从多维度验证了所提优化策略的有效性和实用性。最后，在评测层面，我们提出了新的数学题评测指标——ProcessInferenceScore，并通过实验验证了该指标的有效性。
